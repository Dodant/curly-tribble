{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b6fa50-550f-4124-a03b-c6034f750631",
   "metadata": {},
   "source": [
    "1. 파이토치 제공 데이터 사용\n",
    "2. 같은 클래스 별 폴더 이미지 데이터 이용\n",
    "3. 개인 데이터 사용 (2 types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a9e8f88-2fc2-4010-b163-95f83d54a311",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp38-cp38-macosx_10_9_x86_64.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/Dodanto/Documents/GitHub/curly-tribble/lib/python3.8/site-packages (from torchvision) (1.21.0)\n",
      "Requirement already satisfied: torch==1.9.0 in /Users/Dodanto/Documents/GitHub/curly-tribble/lib/python3.8/site-packages (from torchvision) (1.9.0)\n",
      "Collecting pillow>=5.3.0\n",
      "  Downloading Pillow-8.3.1-cp38-cp38-macosx_10_10_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /Users/Dodanto/Documents/GitHub/curly-tribble/lib/python3.8/site-packages (from torch==1.9.0->torchvision) (3.10.0.0)\n",
      "Installing collected packages: pillow, torchvision\n",
      "Successfully installed pillow-8.3.1 torchvision-0.10.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tr\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d035552-78f0-4b48-80b8-93a614a14ab7",
   "metadata": {},
   "source": [
    "## 1. 파이토치 제공 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a3e569c-f106-4076-abd8-fcab49bbb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = tr.Compose([tr.Resize(8), tr.ToTensor()])\n",
    "# Transforms on PIL Image\n",
    "# Pad, Grayscale, RandomCrop, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa52a5b4-cebc-4fdd-9447-486324fc8e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transf)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb64e4b2-24c2-454a-a7a7-d082517633ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 8])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd25684f-8476-4ebb-ad06-30a654dad9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=50, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0de7867-5ee7-4b0a-b8bb-f8e22ad4d595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "704f840d-52d3-41d8-a056-407e2f130013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 8, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b31a68-1848-4927-8c7c-80bd522fa915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "569a618d-77e5-4397-a983-31de1d8e62a2",
   "metadata": {},
   "source": [
    "## 2. 같은 클래스 별 폴더 이미지 데이터 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45309578-5b93-4032-8df1-814ef788d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./class/tiger   ./class/lion\n",
    "transf = tr.Compose([tr.Resize(8), tr.ToTensor()])\n",
    "trainset = torchvision.datasets.ImageFolder(root='./class', transform=transf)\n",
    "trainloader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=2)\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b83f1-6efa-4e0a-a1e4-30ec2dc2e3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "957e8bc4-2f4a-4c00-aa0e-7df6e118ec9f",
   "metadata": {},
   "source": [
    "## 3. 개인 데이터 사용 (2 types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69362561-a797-4979-8f93-6c827f6b12e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 32, 32, 3) (20, 1)\n"
     ]
    }
   ],
   "source": [
    "# import preprocessing\n",
    "train_images = np.random.randint(256, size=(20,32,32,3))\n",
    "train_labels = np.random.randint(2, size=(20,1))\n",
    "\n",
    "# preprocessing\n",
    "# train_images, train_labels = preprocessing(train_images, train_labels)\n",
    "\n",
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f309cb7c-6f2c-47c1-a0cc-e97f44c47a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorData(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.x_data = self.x_data.permute(0,3,1,2) ## 이미지 개수, 채널 수, 너비, 높이\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cef3df62-fe5f-4531-9cf6-a6ebec77c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorData(train_images, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0e5ea48-db7c-4f26-b5d6-36961f8d475e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ed7a8a46-d281-450c-96a2-3c54eb86071c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 32, 32])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3011a8c1-986f-4b98-b775-34716791fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "class ToTensor:\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        \n",
    "class LinearTensor:\n",
    "    def __init__(self, slope, bias):\n",
    "        \n",
    "    def __call__(self, sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd922c-b466-4f0e-842e-90c4d9a5e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __call__(self):\n",
    "        ...\n",
    "        transf = tr.Compose([tr.ToPILImage(), ...])\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
